{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (Bayes Ingenuo)\n",
    "\n",
    "Los clasificadores de Bayes ingenuos son clasificadores lineales conocidos por su simpleza y eficiencia. El modelo probabilístico de los clasificadores de Bayes ingenuos se basa en el Teorema de Bayes y el adjetivo *ingenuo* viene de la suposición  de que las características en la base de datos son mutuamente independientes.\n",
    "\n",
    "En la práctica, la suposición de independencia se viola frecuentemente, pero el clasificador de Bayes ingenuo tiende a tener un buen desempeño aún bajo esta suposición poco realista, especialmente para tamaños pequeños de muestra.\n",
    "\n",
    "Algunos campos en los que se usa el clasificador Bayesiano ingenuo son: diagnóstico de enfermedades, clasificación de secuencias de RNA y filtrado de spam en clientes de correo.\n",
    "\n",
    "Con el fin de entender mejor el trabajo de los clasificadores Bayesianos ingenuos se deben tener en cuenta los siguientes conceptos.\n",
    "\n",
    "1.***Probabilidad a posteriori***\n",
    "\n",
    "La probabilidad a posteriori está definida por el teorema de Bayes de la siguiente manera:\n",
    "\n",
    "$$ P_{a-posteriori} = \\frac{P_{condicional} \\times P_{a-priori}}{evicencia}$$\n",
    "\n",
    "La probabilidad a posteriori puede  definirse en el contexto de la clasificación como *¿Cuál es la probabilidad de un objeto particular de pertenecer a la clase $i$ dados sus valores de características observados?*\n",
    "\n",
    "*Ejemplo*\n",
    "\n",
    "¿Cuál es la probabilidad de que una persona tenga diabetes dado cierto valor de glucosa medida en la sangre pre y post?, i.e.:\n",
    "\n",
    "$$P(diabetes|\\mathbf{x}_i), \\quad \\mathbf{x}_i=[90mg/dl,145mg/dl],$$\n",
    "\n",
    "donde:\n",
    "\n",
    "* $\\mathbf{x}_i$ es el vector de características de la obsrvación $i$, con $i=1,2,\\ldots,n$\n",
    "* $\\omega_j$ la notación de clase $j$, con $j=1,2,\\ldots,m$\n",
    "* $P(\\mathbf{x}_i|\\omega_j)$ la probabliliad de observar la muestra $\\mathbf{x}_i$ dado que pertenece a la clase $\\omega_j$.\n",
    "\n",
    "La notación general de la probabilidad a posteriori se puede escribir como:\n",
    "\n",
    "$$P(\\omega_j|\\mathbf{x}_i)=\\frac{P(\\mathbf{x}_i|\\omega_j)P(\\omega_j)}{P(\\mathbf{x}_i)}$$\n",
    "\n",
    "La función objetivo en la probabilidad de Bayes ingenuo, es maximizar la probabilidad a posteriori dado el conjunto de entrenamiento, en orden de formular la regla de decisión.\n",
    "\n",
    "$$\\omega_{predict}\\leftarrow \\arg\\max_{j=1,\\ldots,m}P(\\omega_j|\\mathbf{x}_i)$$.\n",
    "\n",
    "Continuando con el ejemplo, se podría formular la regla de decisión basada en las probabilidades a posteriori como sigue:\n",
    "\n",
    ">*** La persona tiene diabetes si \n",
    "$$P(diabetes|\\mathbf{x}_i)\\geq P(no-diabetes|\\mathbf{x}_i)$$\n",
    "> si no, la persona es saludable ***\n",
    "\n",
    "2.***Probabilidad de clase condicional***\n",
    "\n",
    "Una suposición que los clasificadores Bayesianos hacen es que las muestras son *i.i.d*. La independencia significa que la probabilidad de una observiación no afecta la probabilidad de otra observación.\n",
    "\n",
    "Una suposición adicional de los clasificadores Bayesianos ingenuos, es la *independencia condicional* de las características. Bajo este supuesto, la probabilidad de clase condicional o verosimilitud de las puestras se puede esttimar directamente del conjunto de entrenamiento evaluando todas las posibilidades de $\\mathbf{x}$. La probabilidad de clase condicional puede calcularse como:\n",
    "\n",
    "$$P(\\mathbf{x}|\\omega_j)=P(x_1|\\omega_j)P(x_2|\\omega_2)\\ldots P(x_p|\\omega_j)=\\prod_{k=1}^{p}P(x_k|\\omega_j)$$\n",
    "\n",
    "Aquí $P(\\mathbf{x}|\\omega_j)$ significa *¿Qué tan probable es observar este patrón particular $\\mathbf{x}$ dado a que pertenece a la clase $\\omega_j$?*. Las verosimilitudes individuales para cada característica pueden estimarse vía estimación de máxima verosimilitud, lo que es una simple fecuencia en el caso de datos categóricos:\n",
    "\n",
    "$$\\hat{P}(x_i|\\omega_j)=\\frac{N_{x_i,\\omega_j}}{N_{\\omega_j}},$$\n",
    "\n",
    "donde $N_{x_i,\\omega_j}$ es el número de veces que la característica $x_i$ aparece en las observaciones de clase $\\omega_j$, y $N_{\\omega_j}$ es el conteo total de todas las característias en la clase $\\omega_j$\n",
    "\n",
    "3.*** Probabilidad a priori***\n",
    "\n",
    "Esta probabilidad se puede interpretar como el conocimiento a priori. En el contexto de la clasificación, la probabilidad a priori también es llamada *prior de clase*, que describe la *probabilidad general de encontrar una clase particular*. Si los priores siguen una distribución uniforme, las probabilidades posteriores estarán determinadas por completo de la probabilidad de clase condicional y de la evidencia.\n",
    "\n",
    "Eventualmente, el conocimiento a priori puede ser determinado  a través del conjunto de entrenamiento, si se asume que los datos de entrenamiento son *i.i.d* y que son una muestra representativa de toda la población. La estimación de máxima verosimilitud puede formularse como:\n",
    "\n",
    "$$\\hat{P}(\\omega_j)=\\frac{N_{\\omega_j}}{N_c},$$\n",
    "\n",
    "donde $N_{\\omega_j}$ es el número de muestras de la clase $\\omega_j$ y $N_c$ todas las muestras.\n",
    "\n",
    "4.***Evidencia***\n",
    "\n",
    "La evidencia $P(\\mathbf{x})$ puede entenderse como la probabilidad de encontrar un patrón particular $\\mathbf{x}$ independientemente de la etiqueta de clase. Usualmente se puede eliminar de la regla de decisión, debido a que suele ser común a todos los términos.\n",
    "\n",
    "Para la implementación de este clasificador tenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #importamos la librería numérica\n",
    "import matplotlib.pyplot as plt #importamos la librería para graficar\n",
    "from sklearn.naive_bayes import GaussianNB #importamos la librería que contiene el clasificador de Bayes\n",
    "from sklearn.model_selection import train_test_split #importamos la librería para la división de la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Data_NaiveBayes.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f8368932b89b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0minput_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Data_NaiveBayes.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Cargamos los datos. Es importante pasar como argumento el delimitador (coma en este caso)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#la matriz de observaciones consta de todas las columnas exceptuando la última. El vector de etiquetas\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#corresponde a la última columna que está en el archivo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding)\u001b[0m\n\u001b[0;32m    924\u001b[0m             \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 926\u001b[1;33m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    927\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'encoding'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m             \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    616\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[0;32m    617\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s not found.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Data_NaiveBayes.txt not found."
     ]
    }
   ],
   "source": [
    "#importamos el archivo de datos que contiene la Base de Dats. El formato es de valores separados por coma.\n",
    "input_file = 'Data_NaiveBayes.txt'\n",
    "#Cargamos los datos. Es importante pasar como argumento el delimitador (coma en este caso)\n",
    "data = np.loadtxt(input_file, delimiter=',')\n",
    "X,y = data[:,:-1],data[:,-1] #la matriz de observaciones consta de todas las columnas exceptuando la última. El vector de etiquetas\n",
    "#corresponde a la última columna que está en el archivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 1**\n",
    "\n",
    "Realiza las gráficas de la matriz de observaciones con todas las posibles combinaciones de las características (e.j. Característica 1 vs Característica 2, Característica 1 vs Característica 3, etc). Asegúrate de que cada una de las clases se vea de un color diferente para saber cuáles características serían mejores para clasificar (las que sean linealmente separables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# espacio para la solución del ejercicio 1\n",
    "x = X[:,0]\n",
    "Y = X[:,1]\n",
    "#colors = (0,0,1)\n",
    "area = np.pi*3\n",
    "\n",
    "# Plot\n",
    "plt.scatter(x, Y, s=area, c=y, alpha=0.5)\n",
    "plt.title('Caracterítica 1 vs Caracterítica 2 ')\n",
    "plt.xlabel('Caracterítica 1')\n",
    "plt.ylabel('Caracterítica 2')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# espacio para la solución del ejercicio 1\n",
    "x = X[:,0]\n",
    "Y = X[:,2]\n",
    "#colors = (0,0,1)\n",
    "area = np.pi*3\n",
    "\n",
    "# Plot\n",
    "plt.scatter(x, Y, s=area, alpha=0.5)\n",
    "plt.title('Caracterítica 1 vs Caracterítica 3 ')\n",
    "plt.xlabel('Caracterítica 1')\n",
    "plt.ylabel('Caracterítica 3')\n",
    "plt.show()\n",
    "\n",
    "x = X[:,0]\n",
    "Y = X[:,3]\n",
    "#colors = (0,0,1)\n",
    "area = np.pi*3\n",
    "\n",
    "# Plot\n",
    "plt.scatter(x, Y, s=area, c=y, alpha=0.5)\n",
    "plt.title('Caracterítica 1 vs Caracterítica 3 ')\n",
    "plt.xlabel('Caracterítica 2')\n",
    "plt.ylabel('Caracterítica 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 2**\n",
    "\n",
    "Particiona la base de datos en entrenamiento y validación con 70% 30%. Preprocesa los datos con remoción de la media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# espacio para la solución del ejercicio 2\n",
    "def remocion(X_train, X_test):\n",
    "    media= X_train.mean(axis=0) #Calcula un vector de medias por columnas (0), osea características. Debe tener el tamaño p\n",
    "    X_train = X_train - media #hace la operacion por cada fila\n",
    "    desviacion = X_train.std(axis=0) #calcula la desviacion estandar\n",
    "    X_train = X_train/desviacion #hacemos la division sobre la desviación \n",
    "    #Realizamos la misma operación sobre el X_test con los datos del X_train\n",
    "    X_test = (X_test - media)/desviacion\n",
    "    return X_train, X_test #retornamos las dos matrices preprocesados\n",
    "#espacio para solucionar el ejercicio 2\n",
    "#Division de la matriz de observaciones\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0 )\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "X_train_prepro, X_test_prepro = remocion(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador = GaussianNB() #instanciamos el clasificador\n",
    "clasificador.fit(X_train_prepro,y_train) #entrenamos el clasificador con la matriz de entranamiento pre-procesada\n",
    "y_predict = clasificador.predict(X_test_prepro) #validamos el clasificador sobre el conjunto de entrenamiento\n",
    "acc = 100.0*(y_test == y_predict).sum()/X_test_prepro.shape[0] #calculamos el acierto de clasificación\n",
    "print('El acierto de clasificación es del ',acc , '%') #imprimimos el acierto de clasificacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 3**\n",
    "\n",
    "Ahora, utiliza como matriz de observaciones $\\mathbf{X}$ el conjunto de todas las observaciones pero **únicamente** con las dos combinaciones de características que consideras mejor separan las clases de acuerdo a las gráficas del **Ejercicio 2**. Preprocesa, particiona y clasifica con esa nueva matriz de observaciones. ¿Funciona mejor o peor el clasificador?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## espacio para solución del ejercicio 3\n",
    "x = range(16)\n",
    "x = np.reshape(x,(4,4))\n",
    "print(x)\n",
    "print(x[[0,3],[:,:]])\n",
    "ft = x[:,1],x[:,2]\n",
    "print(ft)\n",
    "##print(X)\n",
    "##print(X[:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
