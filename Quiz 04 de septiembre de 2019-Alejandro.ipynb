{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 04 de septiembre de 2019\n",
    "\n",
    "## Nombre:\n",
    "\n",
    "Se tiene la base de datos de vino en el link $\\verb|https://archive.ics.uci.edu/ml/datasets/wine|$ que contiene 13 atributos. Se quiere entrenar con ella un clasificador, sin embargo no es posible tener en cuenta los 13 atributos. Se le pide encontrar una estrategia para disminuir la cantidad de atributos, de tal forma que se consiga la máxima clasificación con la mínima cantidad de características. Puede utilizar cualquiera de los clasificadores y cualquiera de las estrategias de preprocesamiento, según lo considere necesario. Debe justificar y explicar **MUY BIEN** la estrategia utilizada y los atributos resultado de la misma. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimum number of features: 12\n",
      "Score with 12 features: 0.843879\n",
      "Int64Index([1, 3, 6, 7, 11, 12], dtype='int64')\n",
      "(178, 6)\n",
      "(124, 6) (54, 6)\n",
      "(124, 6) (54, 6)\n",
      "(54,) (54,)\n",
      "(178, 13) (178,)\n",
      "94.44444444444444\n",
      "El acierto de clasificación para las caracteristicas es del  94.44444444444444 %\n"
     ]
    }
   ],
   "source": [
    "#espacio para solucionar el quiz\n",
    "#importo las librerias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt #importamos la librería para graficar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import genfromtxt\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.naive_bayes import GaussianNB #importamos la librería que contiene el clasificador de Bayes\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "#leo los datos y los particiono\n",
    "data = genfromtxt('wine.data', delimiter=',')\n",
    "\n",
    "X,y = data[:,1:14],data[:,0]\n",
    "\n",
    "#voy a evaluar el metodo para ver como se comporta con todas las caracteristicas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Veo cuantas caracterirsticas son optimas para evaluar en la seleccion de caracterirsticas\n",
    "#no of features\n",
    "nof_list=np.arange(1,13)            \n",
    "high_score=0\n",
    "\n",
    "nof=0           \n",
    "score_list =[]\n",
    "for n in range(len(nof_list)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(model,nof_list[n])\n",
    "    X_train_rfe = rfe.fit_transform(X_train,y_train)\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    model.fit(X_train_rfe,y_train)\n",
    "    score = model.score(X_test_rfe,y_test)\n",
    "    score_list.append(score)\n",
    "    if(score>high_score):\n",
    "        high_score = score\n",
    "        nof = nof_list[n]\n",
    "print(\"Optimum number of features: %d\" %nof)\n",
    "print(\"Score with %d features: %f\" % (nof, high_score))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(X,columns=[1,2,3,4,5,6,7,8,9,10,11,12,13])\n",
    "\n",
    "#df[14] = data.target\n",
    "X_1 = df.drop(13,1)   #Feature Matrix\n",
    "\n",
    "\n",
    "\n",
    "cols = list(X_1.columns)\n",
    "model = LinearRegression()\n",
    "#Initializing RFE model\n",
    "rfe = RFE(model,6)             \n",
    "#Transforming data using RFE\n",
    "X_rfe = rfe.fit_transform(X_1,y)  \n",
    "#Fitting the data to model\n",
    "model.fit(X_rfe,y)              \n",
    "temp = pd.Series(rfe.support_,index = cols)\n",
    "selected_features_rfe = temp[temp==True].index\n",
    "print(selected_features_rfe)\n",
    "\n",
    "X_2 = data[:,selected_features_rfe]\n",
    "print(X_2.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_2,y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "#Clasificadores============================================================\n",
    "print(X_train.shape, X_test.shape)\n",
    "def remocion(X_train, X_test):\n",
    "    media = X_train.mean(axis = 0)\n",
    "    desviacion = X_train.std(axis = 0)\n",
    "    X_train_prepro = (X_train - media)/desviacion\n",
    "    X_test_prepro = (X_test - media)/desviacion\n",
    "    return X_train_prepro, X_test_prepro\n",
    "\n",
    "X_train_preproR, X_test_preproR = remocion(X_train, X_test)\n",
    "print(X_train_preproR.shape,X_test_preproR.shape)\n",
    "#clasificador = LogisticRegression(C = 10000.0, random_state = 0)\n",
    "\n",
    "clasificador = GaussianNB()\n",
    "clasificador.fit(X_train, y_train)\n",
    "y_pred = clasificador.predict(X_test)\n",
    "print(y_pred.shape,y_test.shape)\n",
    "print(X.shape,y.shape)\n",
    "acc = 100.0*(y_test == y_pred).sum()/X_test.shape[0]\n",
    "acierto = np.mean(y_test == y_pred)\n",
    "print(acierto*100)\n",
    "print('El acierto de clasificación para las caracteristicas es del ',acc , '%')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
